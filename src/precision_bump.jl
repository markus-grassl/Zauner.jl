export re_im_proj, precision_bump, precision_bump!, pow_to_elem_sym_poly


@doc """
    re_im_proj(ψ::Vector{Complex{BigFloat}})
    re_im_proj(z::Vector{BigFloat})

\\
Stack the real and imaginary parts of the vector `ψ` and normalize so that 
the first coordinate, assumed nonzero, is normalized to 1 and then dropped. 

\\
If called with type `Vector{BigFloat}` then it does the inverse transformation, taking an even-length real vector to a complex one with unit first coordinate. 

# Examples
A simple example:
```jldoctest
v = Complex{BigFloat}.([ 1; im; -1; -im])
re_im_proj(v)

# output

6-element Vector{BigFloat}:
  0.0
 -1.0
  0.0
  1.0
  0.0
 -1.0
```
"""
re_im_proj(ψ::Vector{Complex{BigFloat}}) = [real.(ψ./ψ[1])[2:end]; imag.(ψ./ψ[1])[2:end]]

function re_im_proj(z::Vector{BigFloat})
    @assert length(z) % 2 == 0    
    [1; z[1:end÷2]] .+ im * [0; z[(1+end÷2):end]]
end



# Take the real and imaginary parts with projective normalization and 
# return the overlap function for a ghost fiducial.
# NOTE: This function has an eight-fold symmetry as a function of (p,q)
# generated by the transformations:
#    (p,q) → (-p,-q)
#    (p,q) → ( q, p)
#    (p,q) → ( p,-q)*
# where the last one means apply the symmetry then complex conjugate the result.

function _ghost_olp_func(z,r,p,q)
    T = eltype(z)
    d  = 1+length(z)÷2
    x0 = [  one(T); z[1:d-1] ] # real part of ψ
    y0 = [ zero(T); z[d:end]]  # imag part of ψ
    xi = circshift(reverse(x0),1)
    yi = circshift(reverse(y0),1)
    xp = circshift(xi,-p)
    yp = circshift(yi,-p)
    xq = circshift(xi,-q)
    yq = circshift(yi,-q)
    xr = circshift(x0,-p-q)
    yr = circshift(y0,-p-q)
    
    # real and imaginary parts.
    if r == 0
        # real part
        sum((xq .* xr + yq .* yr) .* (x0 .* xp + y0 .* yp) + 
            (xr .* yq - xq .* yr) .* (xp .* y0 - x0 .* yp) ) - 
        ((p==0) + (q==0))/(d+one(T)) * sum(xi.*x0 + yi.*y0)^2
    elseif r==1
        # imaginary part
        if ( p==0 || q == 0 || 2p == d || 2q == d )
            zero(T) # these values are manifestly real and don't contribute.
        else
            sum((xq .* yr - yq .* xr) .* (x0 .* xp + y0 .* yp) + 
                (yr .* yq + xq .* xr) .* (xp .* y0 - x0 .* yp) )
        end
    end
end


# both real and imaginary parts on a combined index
_ghost_olp_func(z,n::Integer) = _ghost_olp_func(z, radix(n, [2,1+(length(z)÷2),1+(length(z)÷2)])... )

# list over a vector
_ghost_olp_func(z,v::AbstractVector) = [ _ghost_olp_func(z,n) for n in v ]


function _ghost_olp_func(z)
    d = 1+length(z)÷2
    #=
        *Empirically*, the following list is sufficient to get a 
        square and full-rank Jacobian. There is room to optimize this.
        *No check* is done to ensure that the Jacobian is full rank, 
        nor is any effort spent on optimizing the condition number through
        a judicious choice of rows. However, after taking the symmetries of 
        the function into account, and dropping the (p,q) == (0,0) equation,
        these are the lexigraphically first independent terms in the real 
        and imaginary parts. It is thus plausible that they should give a 
        full-rank Jacobian, at least for generic perturbations. 
    =#
    v = [ [            q for q=1:d÷2];        # real part, first row
          [        d + q for q=1:d÷2];        # real part, second row
          [       2d + q for q=2:d÷2];        # real part, third row
          [ d^2 +  d + q for q=1:(d-1)÷2];    # imag part, second row
          [ d^2 + 2d + q for q=2:(1-(-1)^d)]  # imag part, one element in the third row for odd d
        ]
    _ghost_olp_func(z,v)
end


# now we do the same thing for standard overlaps

function _olp_func(z,r,p,q)
    T = eltype(z)
    d  = 1+length(z)÷2
    x0 = [  one(T); z[1:d-1] ] # real part of ψ
    y0 = [ zero(T); z[d:end]]  # imag part of ψ
    xp = circshift(x0,-p)
    yp = circshift(y0,-p)
    xq = circshift(x0,-q)
    yq = circshift(y0,-q)
    xr = circshift(x0,-p-q)
    yr = circshift(y0,-p-q)
    
    # real and imaginary parts.
    if r == 0
        # real part
        sum((xq .* xr + yq .* yr) .* (x0 .* xp + y0 .* yp) + 
            (xr .* yq - xq .* yr) .* (xp .* y0 - x0 .* yp) ) - 
        ((p==0) + (q==0))/(d+one(T)) * sum(x0.*x0 + y0.*y0)^2
    elseif r==1
        # imaginary part
        if ( p==0 || q == 0 || 2p == d || 2q == d )
            zero(T) # these values are manifestly real and don't contribute.
        else
            sum((xq .* yr - yq .* xr) .* (x0 .* xp + y0 .* yp) + 
                (yr .* yq + xq .* xr) .* (xp .* y0 - x0 .* yp) )
        end
    end
end


# both real and imaginary parts on a combined index
_olp_func(z,n::Integer) = _olp_func(z, radix(n, [2,1+(length(z)÷2),1+(length(z)÷2)])... )

# list over a vector
_olp_func(z,v::AbstractVector) = [ _olp_func(z,n) for n in v ]


function _olp_func(z)
    d = 1+length(z)÷2
    #=
        *Empirically*, the following list is sufficient to get a 
        square and full-rank Jacobian. There is room to optimize this.
        *No check* is done to ensure that the Jacobian is full rank, 
        nor is any effort spent on optimizing the condition number through
        a judicious choice of rows. However, after taking the symmetries of 
        the function into account, and dropping the (p,q) == (0,0) equation,
        these are the lexigraphically first independent terms in the real 
        and imaginary parts. It is thus plausible that they should give a 
        full-rank Jacobian, at least for generic perturbations. 
    =#
    v = [ [            q for q=1:d÷2];        # real part, first row
          [        d + q for q=1:d÷2];        # real part, second row
          [       2d + q for q=2:d÷2];        # real part, third row
          [ d^2 +  d + q for q=1:(d-1)÷2];    # imag part, second row
          [ d^2 + 2d + q for q=2:(1-(-1)^d)]  # imag part, one element in the third row for odd d
        ]
    _olp_func(z,v)
end




@doc raw"""
    precision_bump(ψ::Vector{Complex{BigFloat}}, prec::Integer [; base::Integer = 10, verbose::Bool = true])

Attempt to use Newton's method to improve the precision of `ψ` to at least `prec` digits in base `base`. 
"""
function precision_bump(ψ::Vector{Complex{BigFloat}}, prec::Integer; base::Integer = 10, verbose::Bool = true)
    z = re_im_proj(ψ)
    precision_bump!(z, prec; base, verbose)
    re_im_proj(z)
end



@doc raw"""
    precision_bump!(z::Vector{BigFloat}, prec::Integer [; base::Integer = 10, verbose::Bool = true])

Attempt to use Newton's method to improve the precision of `z` to at least `prec` digits in base `base`, 
where `z` is the real projective representation of `ψ`. 
"""
function precision_bump!(z::Vector{BigFloat}, prec::Integer; base::Integer = 2, verbose::Bool = true)
    @assert base > 1 "base must be an integer ≥ 2"
    if base == 2
        basename = "bits"
    elseif base == 10
        basename = "digits"
    else
        basename = "digits base $base"
    end
    verbose && println("Increase ghost precision...")
    digits = floor( Int, -log( base, maximum(abs.(_ghost_olp_func(z)))) )
    while digits < prec
        setprecision( BigFloat, 2*digits; base = base)
        verbose && println("Current ghost precision is $digits $basename.")
        # Run an iteration of Newton's method
        if verbose
            @time z .-= jacobian(_ghost_olp_func, z)\_ghost_olp_func(z)
        else
            z .-= jacobian(_ghost_olp_func, z)\_ghost_olp_func(z)
        end
        digits = floor( Int, -log( base, maximum(abs.(_ghost_olp_func(z)))) )
    end
    verbose && println("Precision of BigFloat is now ", precision(BigFloat; base = base), " $basename.")
    verbose && println("Final ghost precision is $digits $basename.")
    return z
end


# Here is a version that allows for a function input. 
# This can be used with _olp_func to improve the precision of a SIC
function precision_bump!(z::Vector{BigFloat}, f::Function, prec::Integer; base::Integer = 2, verbose::Bool = true)
    @assert base > 1 "base must be an integer ≥ 2"
    if base == 2
        basename = "bits"
    elseif base == 10
        basename = "digits"
    else
        basename = "digits base $base"
    end
    verbose && println("Increase precision...")
    digits = floor( Int, -log( base, maximum(abs.(f(z)))) )
    digits = maximum([digits; 32]) # minimum precision is single-float, hard-coded
    while digits < prec
        setprecision( BigFloat, 2*digits; base = base)
        verbose && println("Current precision is $digits $basename.")
        # Run an iteration of Newton's method
        if verbose
            @time z .-= jacobian(f, z)\f(z)
        else
            z .-= jacobian(f, z)\f(z)
        end
        digits = floor( Int, -log( base, maximum(abs.(f(z)))) )
        digits = maximum([digits; 32]) # minimum precision is single-float
    end
    verbose && println("Precision of BigFloat is now ", precision(BigFloat; base = base), " $basename.")
    verbose && println("Final precision is $digits $basename.")
    return z
end




@doc raw"""
    pow_to_elem_sym_poly( p::AbstractVector) 

\\
Take an abstract vector `p` and apply the recursion relation for 
converting power sums to elementary symmetric polynomials assuming 
that `p[1]` is the power sum of degree 1. 
Output `e` is indexed so that `e[1] == 1` is the zeroth elementary 
symmetric polynomial and `length(e) == length(p)+1`. 
"""
function pow_to_elem_sym_poly(p::AbstractVector)
    L = length(p)
    esp = copy(p)
    pushfirst!( esp, one(eltype(p))) # e0 = 1
    for k=1:L
        esp[k + 1] = sum( [ (-1)^(j-1)*esp[k-j+1]*p[j]/k for j=1:k] )
    end
    esp
end
